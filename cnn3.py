import tensorflow as tf
from data_reader import load_data
import numpy as np
from uncompress import *
import os

slim = tf.contrib.slim

def lrelu(alpha):
    def op(inputs):
        return tf.maximum(alpha * inputs, inputs, name='leaky_relu')
    return op

def conv_net(input):

    with slim.arg_scope([slim.conv2d, slim.fully_connected], #using scope to avoid mentioning the paramters repeatdely
                                        activation_fn=lrelu(0.005),
                                        weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),
                                        weights_regularizer=slim.l2_regularizer(0.0005)):
	#net = slim.max_pool2d(input,(1,4),(1,4), padding='VALID', scope='pool_0')
	

        net = slim.conv2d(input, 512, (5,86796), 1, padding='SAME', scope='conv_1')
                        
        net = slim.max_pool2d(net, (4,1),4, padding='VALID', scope='pool_2')
                        
        net = slim.conv2d(net, 512, (5,1), 1, scope='conv_3')

        net = slim.max_pool2d(net, (4,1),4, padding='VALID', scope='pool_4')

	net = slim.conv2d(net, 1024, (5,1), 1, scope='conv_4')

        net = slim.flatten(net, scope='flatten_5')

	'''net = slim.fully_connected(net, 1024, scope='fc_6',activation_fn=tf.nn.softmax)
	
	net = slim.fully_connected(net, 256, scope='fc_7',activation_fn=tf.nn.softmax)

        net = slim.fully_connected(net, 2, scope='fc_8',activation_fn=tf.nn.softmax)'''
	
	net = slim.fully_connected(net, 4096, scope='fc5')
	net = slim.dropout(net, 0.5, scope='dropout6')
	net = slim.fully_connected(net, 4096, scope='fc7')
	net = slim.dropout(net, 0.5, scope='dropout8')
	net = slim.fully_connected(net,2, activation_fn=None, scope='fc9')

    
    return net

def one_hot(batch_size,Y):

    B = np.zeros((batch_size,2))

    B[np.arange(batch_size),Y] = 1

    return B

if __name__=='__main__':

    #os.environ['CUDA_VISIBLE_DEVICES'] = ''
    # print one_hot(3,np.array((1,0,1)))
    # exit(0)
    # Training Parameters
    learning_rate = 0.00001
    num_epoch = 5
    batch_size = 2
    display_step = 1
    input_size = 50
    num_classes = 2 

  

    X = tf.placeholder(tf.float32, [None, input_size,86796,1])
    Y = tf.placeholder(tf.float32, [None, num_classes])
    #logits = conv_net(X)
    #prediction = tf.nn.softmax(logits)
    prediction = conv_net(X)
    # Define loss and optimizer
    '''loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
	logits=logits, labels=Y))'''
    loss_op = slim.losses.softmax_cross_entropy(prediction, Y)

    tf.summary.scalar('loss',loss_op)
    optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)
    train_op = optimizer.minimize(loss_op)


    # Evaluate model
    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
    tf.summary.scalar('accuracy',accuracy)

    # Initialize the variables (i.e. assign their default value)
    init = tf.global_variables_initializer()

    data_X,data_Y = load_data()

    indices = np.random.permutation(np.arange(data_X.shape[0]))

    data_X = data_X[indices,:,:]

    data_Y = data_Y[indices]

    

    merged = tf.summary.merge_all()
    saver = tf.train.Saver()

    with tf.Session() as sess:

	train_writer = tf.summary.FileWriter("cnn_logs_8/",
	                              sess.graph)

	# Run the initializer
	sess.run(init)

	'''

	print 'restoring session'
	saver.restore(sess, "logs3/epoch0i180.ckpt")
	print 'done loading'
	# exit(0) '''

	i = 0
	print 'started training'
	for epoch in range(num_epoch):
	    for step in range(data_X.shape[0]/batch_size):
	        batch_x, batch_y = data_X[step*batch_size:(step+1)*batch_size],\
	        data_Y[step*batch_size:(step+1)*batch_size]

	        

	        batch_x = uncompress(batch_x,86796)

	        # print batch_y
	        batch_y = one_hot(batch_size,batch_y)

	        batch_y = np.repeat(batch_y,50,axis=0)

	        # print batch_y

	        assert(batch_x.shape[0]==batch_y.shape[0])

	        # print batch_x.shape
	        # print batch_y.shape

	        # exit(0)

	        # Run optimization op (backprop)
	        _,summary = sess.run([train_op,merged], feed_dict={X: batch_x, Y: batch_y})
	        train_writer.add_summary(summary, i)
	        if step % display_step == 0:
	            # Calculate batch loss and accuracy
	            loss, acc,summary = sess.run([loss_op, accuracy,merged], feed_dict={X: batch_x,
	                                                                Y: batch_y})
	            print("LR : "+str(learning_rate)+" Epoch : " + str(epoch) + " Step " + str(step) + ", Minibatch Loss= " + \
	                "{:.4f}".format(loss) + ", Training Accuracy= " + \
	                "{:.3f}".format(acc))
	            
	            # train_writer.add_summary(summary, step)
	        
	        if i%20 == 0:

	            print 'saving checkpoint'
	            save_path = saver.save(sess, os.path.join('cnn_logs_8','epoch'+str(epoch)+\
	            'i'+str(i)+'.ckpt'))
	            print("Model saved in path: %s" % save_path)
	        
	        i+=1

        # print("Optimization Finished!")
